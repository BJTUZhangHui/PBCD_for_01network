{"cells":[{"cell_type":"markdown","metadata":{"id":"tT2VMZJXYhTw"},"source":["'''\n","1. Version  : 1\n","2. Time     : 5th May\n","3. Author   : Hui Zhang\n","4. Function : PBCD method to solve 2 hidden layer 0/1 DNN on MNIST\n","5. Structure: 2 hidden layer with 1000 nodes in each layer\n","6. Relation paper: PBCD method for 0/1 DNN\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15094,"status":"ok","timestamp":1690682227458,"user":{"displayName":"Zhang Hui","userId":"06808414998828253018"},"user_tz":-480},"id":"5L1Ah48Ll3Nu","outputId":"e001d02b-cbac-48f0-d6ed-6e9bdd476785"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","# drive.mount(\"/content/drive/\", force_remount=True)"]},{"cell_type":"markdown","metadata":{"id":"CXfp0lN9YW3v"},"source":["# **Define relavant function and load data**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kh7W8cowv1-6"},"outputs":[],"source":["import numpy as np   # science calculate  instill numpy\n","import struct  # y???????\n","from sklearn.utils import shuffle  # instill scikit-learn/ scikit\n","import time\n","import os  # o??????????\n","import scipy\n","import torch as tc\n","import tensorflow as tf\n","import matplotlib.dates as mdates\n","import matplotlib.pyplot as plt\n","\n","from tqdm import tqdm, trange\n","import tensorflow.keras as keras\n","from tensorflow.keras import datasets\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.layers import Dense,Conv2D,Dropout,Flatten,Activation,BatchNormalization,AveragePooling2D,MaxPooling2D\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.model_selection import train_test_split\n","\n","# Standardized data in 0-1\n","def normalize_data(ima):\n","    a_max = np.max(ima)  # 图像矩阵的最大值\n","    a_min = np.min(ima)  # 图像矩阵的最小值\n","    for i in range(ima.shape[0]):  # 矩阵第二维的长度：如3*4矩阵这个就是3\n","        for j in range(ima.shape[1]):  # numpy.core.fromnumeric 中的函数 第一维的长度\n","            ima[i][j] = (ima[i][j] - a_min) / (a_max - a_min)\n","    return ima  # 返回是一个矩阵\n","\n","# predeal dataset\n","def create_model(directory):\n","    # chanDim = -1\n","    input_shape = (28, 28, 1)\n","    model_original = load_model(os.path.join(directory, \"prenetwork/99.4_CNN.h5\"))\n","\n","    model = Sequential()\n","    # Layer 1 Conv2D\n","    cnn_layer = Conv2D(filters = 32, kernel_size = 5, strides = 1, activation = 'relu', input_shape = input_shape)\n","    cnn_layer.trainable=False\n","    model.add(cnn_layer)\n","    cnn_layer2= Conv2D(filters = 32, kernel_size = 5, strides = 1, use_bias=False)\n","    cnn_layer2.trainable=False\n","    model.add(cnn_layer2)\n","    cnn_layer3=BatchNormalization()\n","    cnn_layer3.trainable=False\n","    model.add(cnn_layer3)\n","    model.add(Activation('relu'))\n","    model.add(MaxPooling2D(pool_size = 2, strides = 2))\n","    model.add(Dropout(0.25,name = 'representation')) ############## 需要这一层的输出\n","    # Layer 3 Conv2D\n","    cnn_layer4= Conv2D(filters = 64, kernel_size = 3, strides = 1, activation = 'relu')\n","    cnn_layer4.trainable=False\n","    model.add(cnn_layer4)\n","    cnn_layer5= Conv2D(filters = 64, kernel_size = 3, strides = 1, use_bias=False)\n","    cnn_layer5.trainable=False\n","    model.add(cnn_layer5)\n","    cnn_layer6= BatchNormalization()\n","    cnn_layer6.trainable=False\n","    model.add(cnn_layer6)\n","    # Layer 4 Pooling Layer\n","    model.add(Activation('relu'))\n","    model.add(MaxPooling2D(pool_size = 2, strides = 2))\n","    model.add(Dropout(0.25))\n","    model.add(Flatten(name='flatten'))\n","\n","\n","    model.add(Dense(500, use_bias=False))\n","    model.add(BatchNormalization())\n","    model.add(Activation('relu'))\n","    model.add(Dense(300, use_bias=False))\n","    model.add(BatchNormalization())\n","    model.add(Activation('relu'))\n","    model.add(Dense(100, use_bias=False))\n","    model.add(BatchNormalization())\n","    model.add(Activation('relu'))\n","    model.add(Dropout(0.25))\n","    model.add(Dense(20, use_bias=False))\n","    model.add(BatchNormalization())\n","    model.add(Activation('relu'))\n","\n","    model.add(Dense(10, activation='softmax'))\n","    model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n","    model.layers[0].set_weights(model_original.layers[0].get_weights())\n","    model.layers[1].set_weights(model_original.layers[1].get_weights())\n","    model.layers[2].set_weights(model_original.layers[2].get_weights())\n","    model.layers[6].set_weights(model_original.layers[6].get_weights())\n","    model.layers[7].set_weights(model_original.layers[7].get_weights())\n","    model.layers[8].set_weights(model_original.layers[8].get_weights())\n","    return model\n","\n","\n","# 初始化各个参数 n_x x的维数 n_h1：第一隐层节点数（靠近x的）, n_h2：第二隐层节点数, n_y： 输出y的维数\n","def initialize_with_zeros(n_x, n_h1, n_h2, n_y):\n","\n","    W1 = tf.random.uniform([n_x, n_h1], minval = -np.sqrt(6) / np.sqrt(n_x + n_h1), maxval = np.sqrt(6) / np.sqrt(n_h1 + n_x), dtype=tf.dtypes.float32, seed=None, name=None)\n","    W2 = tf.random.uniform([n_h1, n_h2], minval = -np.sqrt(6) / np.sqrt(n_h1 + n_h2), maxval = np.sqrt(6) / np.sqrt(n_h1 + n_h2), dtype=tf.dtypes.float32, seed=None, name=None)\n","    W3 = tf.random.uniform([n_h2, n_y], minval = -np.sqrt(6) / np.sqrt(n_y + n_h2), maxval = np.sqrt(6) / np.sqrt(n_y + n_h2), dtype=tf.dtypes.float32, seed=None, name=None)\n","    dW1_1 = tf.zeros((n_x, n_h1))\n","    dW1_2 = tf.zeros((n_x, n_h1))\n","    dW2_1 = tf.zeros((n_h1, n_h2))\n","    dW2_2 = tf.zeros((n_h1, n_h2))\n","    dW3_1 = tf.zeros((n_h2, n_y))\n","    dW3_2 = tf.zeros((n_h2, n_y))\n","\n","    g_sum = {\"dW1_1\": dW1_1,\n","             \"dW1_2\": dW1_2,\n","             \"dW2_1\": dW2_1,\n","             \"dW2_2\": dW2_2,\n","             \"dW3_1\": dW3_1,\n","             \"dW3_2\": dW3_2\n","             }\n","    parameters = {\"W1\": W1,\n","                  \"W2\": W2,\n","                  \"W3\": W3,\n","                  \"idx1\": np.nonzero(tf.ones((n_x, 1)) )[0],\n","                  \"idx2\": np.nonzero(tf.ones((n_h1, 1)) )[0],  # initial\n","                  \"idx3\": np.nonzero(tf.ones((n_h2, 1)) )[0]  # initial\n","                  }\n","    pa_in = 1.e-7 * np.ones((4, 1))  # 每一层罚参数初始值\n","    pa_in[3] = 1 / (2 * batch_size)  # 每一层罚参数初始值\n","    tau_in = 1.e-6 * np.ones((4, 1))  # 每一层罚参数初始值\n","    return parameters, pa_in, tau_in, g_sum\n","\n","\n","# 存储参数\n","def save_parameters(parameters, directory):\n","    for key, val in parameters.items():\n","        if not os.path.exists(os.path.join(directory, str(key) + '.npy')):\n","          os.makedirs(os.path.join(directory, str(key) + '.npy'))\n","        tf.save(os.path.join(directory, str(key) + '.npy'), val)\n","\n","\n","def costloss(V3, Y):\n","    cost = tf.square(tf.norm(V3 - Y)) / 2\n","    return cost\n","\n","def penaltyloss(P, C, X, Y):\n","    cost1 = pa[0] * tf.square( tf.norm(Y - hardmax(C[\"U3\"])) )\n","    cost2 = tau[3] * tf.square(tf.norm( C[\"U3\"] - tf.matmul(tf.transpose(P['W3']),  C[\"V2\"]))  )\n","    cost3 = pa[2] * tf.square( tf.norm(C[\"V2\"] - sgn(C[\"U2\"])) ) + tau[2] * tf.square( tf.norm(C[\"U2\"] - tf.matmul(tf.transpose(P['W2']), C[\"V1\"])) )\n","    cost4 =  pa[1] * tf.square( tf.norm(C[\"V1\"] - sgn(C[\"U1\"])) ) + tau[1] * tf.square( tf.norm(C[\"U1\"] - tf.matmul(tf.transpose(P['W1']), X)) )\n","    cost5 = gam * (tf.square( tf.norm(P['W3'])) + tf.square(tf.norm(P['W2'])) + tf.square(tf.norm(P['W1'])))\n","\n","    cost = {\"cost1\": cost1.numpy(),\n","             \"cost2\": cost2.numpy(),\n","             \"cost3\": cost3.numpy(),\n","             \"cost4\": cost4.numpy(),\n","             \"cost5\": cost5.numpy(),\n","             \"costall\": cost1.numpy() + cost2.numpy() + cost3.numpy()+cost4.numpy()+cost5.numpy()\n","             }\n","    return cost\n","\n","def sgn(x):\n","    s = tf.sign(x * tf.cast(x > 0, dtype=tf.float32))\n","    return s\n","\n","\n","def softmax(x):  # 返回第一个最大值的位置\n","    v = tf.argmax(x)\n","    return v\n","\n","def hardmax(x):  # 返回矩阵值 dimision: 10 * 64\n","    v1 = np.zeros(x.shape)\n","    v = np.argmax(x, 0)  # tf.argmax(a,1) 每一列最大值位置\n","    for i in range(x.shape[1]):\n","        v1[v[i], i] = 1\n","\n","    return tf.convert_to_tensor(v1, dtype=tf.float32)\n","\n","# 将原矩阵拉成784列但不知道多少行（-1）的矩阵（np.会自动计算配套维数）\n","def image2vector(image):\n","    v = np.reshape(image, [-1, n_x])\n","    return v.T\n","\n","\n","def mini_batchs_generator(inputs, targets, batch_size):\n","    inputs_data_size = len(inputs)\n","    targets_data_size = len(targets)\n","    # print(\"len(inputs):={}, size of inputs {}, size of targets{}\".format(inputs_data_size, inputs.shape, targets.shape))\n","    assert inputs_data_size == targets_data_size, \"The length of inputs({}) and targets({}) must be consistent\".format(\n","        inputs_data_size, targets_data_size)\n","\n","    shuffled_input, shuffled_target = shuffle(inputs, targets)  # 元素随机排序？\n","    mini_batches = [(shuffled_input[k: k + batch_size], shuffled_target[k: k + batch_size])\n","                    for k in range(0, inputs_data_size, batch_size)]  #\n","    for x, y in mini_batches:\n","        # print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n","        # print(x.shape)\n","        yield x, y  # yield = return的功能 + 未终止再使用直到循环完毕(实现一个epoch功能)\n","\n","def produce_step_adam(grad_1, grad_2, g_new, epsilon, delta, rho_1, rho_2, step_ad):\n","    \"\"\"\n","    adam\n","     (i) 梯度滑动-一阶矩  g = rho_1 * grad_1 + (1-rho_1)*g_new\n","     (ii)二阶矩  r_t = rho_2 * grad + (1-rho_1)*g_new*g_new\n","     (iii)更新  step = epsilon /sqrt(delta+r_t)\n","    \"\"\"\n","    grad_1 = rho_1 * grad_1 + (1 - rho_1) * g_new\n","    g = grad_1 / (1 - pow(rho_1, step_ad))\n","    r_t = rho_2 * grad_2 + (1 - rho_2) * g_new * g_new\n","    r = r_t / (1 - pow(rho_2, step_ad))\n","    step = epsilon / tf.sqrt(delta + r)\n","    return step, g, grad_1, r_t\n","\n","def forward_propagation_pre(x_tol, parameters_for):\n","    layers_num = 3\n","    cache_for = {}\n","    layer_input = x_tol\n","    for layer in range(1, layers_num + 1):\n","        locals()['A' + str(layer)] = tf.matmul(tf.transpose(parameters_for['W' + str(layer)]),\n","                                            layer_input)\n","        cache_for['A' + str(layer)] = locals()['A' + str(layer)]\n","        locals()['Z' + str(layer)] = locals()['A' + str(layer)] * tf.cast(locals()['A' + str(layer)] > 0, dtype=tf.float32)\n","        layer_input = locals()['Z' + str(layer)]\n","        cache_for['Z' + str(layer)] = locals()['Z' + str(layer)]\n","    return locals()['Z' + str(layers_num)], cache_for\n","\n","def prox_l0(x, sparse, vec_yn):\n","    k = int(x.shape[0] * sp0)\n","    abx = np.linalg.norm(x, axis=1, keepdims=True)\n","    # print(abx.shape, x.shape[0], k)\n","    thre = np.partition(abx, kth=k, axis=None)[k]\n","    abx *= (abx >= thre)\n","    # print(np.partition(x, kth=k, axis=None)[k], np.sqrt(2 * 0.13 * sparse * lr))\n","    # x *= x >= np.sqrt(2 * sparse * lr)\n","    # if k > 5:\n","    #     thre = np.partition(x, kth=k, axis=None)[k]\n","    #     x *= (x >= thre)\n","    return abx\n","\n","def optimizer_adam(parameters, cache, X, Y, g_sum, epsilon, delta, rho_1, rho_2, step_ad, iht=False):\n","    A1 = cache[\"Z1\"]\n","    A2 = cache[\"Z2\"]\n","    A3 = cache[\"Z3\"]\n","\n","\n","    dZ3 = (A3 - Y)  # * Z3(1-Z3)  # h2 * batch\n","    dZ2 = tf.matmul(parameters[\"W3\"], dZ3) * tf.cast(A2 > 0,dtype=tf.float32)\n","    dZ1 = tf.matmul(parameters[\"W2\"], dZ2) * tf.cast(A1 > 0,dtype=tf.float32)\n","\n","    dW3 = tf.matmul((A2), tf.transpose(dZ3))\n","    dW2 = tf.matmul((A1), tf.transpose(dZ2))\n","    dW1 = tf.matmul((X), tf.transpose(dZ1))\n","\n","    step_W3, dW3, g_sum[\"dW3_1\"], g_sum[\"dW3_2\"] = produce_step_adam(g_sum[\"dW3_1\"], g_sum[\"dW3_2\"], dW3,\n","                                                                     epsilon, delta, rho_1, rho_2, step_ad)\n","    step_W2, dW2, g_sum[\"dW2_1\"], g_sum[\"dW2_2\"] = produce_step_adam(g_sum[\"dW2_1\"], g_sum[\"dW2_2\"], dW2,\n","                                                                     epsilon, delta, rho_1, rho_2, step_ad)\n","    step_W1, dW1, g_sum[\"dW1_1\"], g_sum[\"dW1_2\"] = produce_step_adam(g_sum[\"dW1_1\"], g_sum[\"dW1_2\"], dW1, epsilon,\n","                                                                     delta, rho_1, rho_2, step_ad)\n","    parameters[\"W1\"] -=  step_W1 * dW1\n","    ep = 1\n","    if ep > -10:\n","      parameters[\"W3\"] -=  step_W3 * dW3\n","      parameters[\"W2\"] -=  step_W2 * dW2\n","    else:\n","      sp0_pre = 0.7\n","      w3 = parameters[\"W3\"].numpy()\n","      w2 = parameters[\"W2\"].numpy()\n","      ep3 = 0 * w3\n","      ep2 = 0 * w2\n","\n","      W_norm2 = prox_l0(w3, sp0_pre, vec_yn=0)\n","      idx_T = np.nonzero(W_norm2)[0]  # W行模的非0指标集Tk\n","      ep3[idx_T] = w3[idx_T] - step_W3.numpy()[idx_T] * dW3.numpy()[idx_T]\n","\n","      W_norm2 = prox_l0(w2, sp0_pre, vec_yn=0)\n","      idx_T = np.nonzero(W_norm2)[0]  # W行模的非0指标集Tk\n","      ep2[idx_T] = w2[idx_T] - step_W2.numpy()[idx_T] * dW2.numpy()[idx_T]\n","      parameters[\"W2\"] = tf.convert_to_tensor(w2, dtype = tf.float32)\n","      parameters[\"W3\"] = tf.convert_to_tensor(w3, dtype = tf.float32)\n","\n","    return parameters, g_sum\n","\n","\n","# feed forward\n","def forward_propagation(X, parameters):\n","    W1 = parameters[\"W1\"]  # W1 维数 （784, 1000）\n","    W2 = parameters[\"W2\"]\n","    W3 = parameters[\"W3\"]\n","\n","    U1 = tf.matmul(tf.transpose(W1), X)  # + b1 * parameters['gamma1']\n","    # betch_size =16, nx*nx = 784 X为（784，16）, gamma1为nx*nx，X * gamma1的维数和X维数相同\n","    V1 = sgn(U1)  # 实现Relu，V1为 (1000，16)为第1隐层的值\n","    U2 = tf.matmul(tf.transpose(W2), V1)  # + b2    # W2^T 维数 （1000,1000） * parameters['gamma2']\n","    V2 = sgn(U2)  # 实现Relu，V2 (1000，16)为第2隐层的值\n","    U3 = tf.matmul(tf.transpose(W3), V2)  # + b3    # W3^T为 （10，1000） * parameters['gamma3']\n","    V3 = hardmax(U3)  # 输出 维数是10*batch 大小\n","\n","    cache = {\n","        \"V0\": X,\n","        \"U0\": X,\n","        \"U1\": U1,\n","        \"V1\": V1,\n","        \"U2\": U2,\n","        \"V2\": V2,\n","        \"U3\": U3,\n","        \"V3\": V3}\n","\n","    return V3, cache\n","\n","\n","\n","\n","def save_parameters(parameters, directoryin):\n","    for key, val in parameters.items():\n","        np.save(os.path.join(directoryin, str(key)+'.npy'), val)\n","\n","def load_parameters(directoryin):\n","    W1 = np.load(os.path.join(directoryin, 'W1.npy'))\n","    W2 = np.load(os.path.join(directoryin, 'W2.npy'))\n","    W3 = np.load(os.path.join(directoryin, 'W3.npy'))\n","    parameters = {\"idx1\": np.nonzero(np.ones((W1.shape[0], 1)))[0],\n","                  \"idx2\": np.nonzero(np.ones((W2.shape[0], 1)))[0],\n","                  \"idx3\": np.nonzero(np.ones((W3.shape[0], 1)))[0],\n","                  \"W1\": tf.convert_to_tensor(W1, dtype=tf.float32),\n","                  \"W2\": tf.convert_to_tensor(W2, dtype=tf.float32),\n","                  \"W3\": tf.convert_to_tensor(W3, dtype=tf.float32)\n","                  }\n","    return parameters\n","\n","\n","if __name__ == '__main__':\n","    data_name = \"MNIST01\"\n","    path_dict = {1: \"01_BCD\"}\n","    directory = (\"/content/drive/MyDrive/PBCDcode/MNIST_minibatch728/data/\")\n","    if not os.path.exists(directory):\n","        os.makedirs(directory)\n"]},{"cell_type":"markdown","metadata":{"id":"LWK5HBMyk5Bm"},"source":["# Predeal datas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tDYWi3UzdB86"},"outputs":[],"source":["    # (train_images_or, train_labels), (test_images_or, test_labels) = tf.keras.datasets.mnist.load_data()\n","\n","    # # How to train\n","    # # y_train = to_categorical(y_train)\n","    # # y_test = to_categorical(y_test)\n","    # model_CNN = create_model(directory)\n","    # # history = model_CNN.fit(x_train, y_train, batch_size=16, epochs=3, validation_split=0.05)\n","    # # # model specific\n","    # # model_CNN.summary()\n","    # # # How to evaluate\n","    # # loss, acc = model_CNN.evaluate(x_test, y_test, verbose=2)\n","    # # How to feadforward and predict\n","    # # x_test_select = x_test\n","    # # y_true = y_test\n","    # # y_pred = model_CNN.predict(x_test_select)\n","    # # print('predict result: ', y_pred.shape)\n","    # # print('true result: ', y_true.shape)\n","    # layer_name = 'flatten'\n","    # intermediate_layer_model = keras.Model(inputs=model_CNN.input,\n","    #                                       outputs=model_CNN.get_layer(layer_name).output)\n","    # train_images = intermediate_layer_model(train_images_or[0:60000,:,:])\n","    # test_images = intermediate_layer_model(test_images_or)\n","    # train_images = train_images.numpy()\n","    # test_images = test_images.numpy()\n","    # np.save(os.path.join(directory, \"rawdata/train_images.npy\"), train_images)\n","    # np.save(os.path.join(directory, \"rawdata/test_images.npy\"), test_images)\n","    # np.save(os.path.join(directory, \"rawdata/train_labels.npy\"), train_labels)\n","    # np.save(os.path.join(directory, \"rawdata/test_labels.npy\"), test_labels)"]},{"cell_type":"markdown","metadata":{"id":"oUxdmTzZ0uhW"},"source":["# Load dataset#"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lo54UyYqiC7L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1690634154604,"user_tz":-480,"elapsed":3007,"user":{"displayName":"Zhang Hui","userId":"06808414998828253018"}},"outputId":"7161338c-e646-438c-ca7f-a6f698753fcb"},"outputs":[{"output_type":"stream","name":"stdout","text":["(60000, 576)\n"]}],"source":["# (train_images_or, train_labels), (test_images_or, test_labels) = tf.keras.datasets.mnist.load_data()\n","train_images = np.load(os.path.join(directory, \"rawdata/train_images.npy\"))\n","test_images = np.load(os.path.join(directory, \"rawdata/test_images.npy\"))\n","train_labels = np.load(os.path.join(directory, \"rawdata/train_labels.npy\"))\n","test_labels = np.load(os.path.join(directory, \"rawdata/test_labels.npy\"))\n","# normal parameters\n","if 1:\n","    print(train_images.shape)\n","    n_x = train_images.shape[1]  # 764--> 578\n","    n_h1 = 2000  # 第一隐层\n","    n_h2 = 2000  # 第二隐层\n","    n_y = 10  # 输出层\n","    # adam parameters\n","    epsilon = 1.e-3  # adam步长的分子\n","    delta = 1.e-8  # adam步长的第一个分母\n","    rho_1 = 0.9  # adam 一阶距滑动系数\n","    rho_2 = 0.99  # adam 二阶距滑动系数\n","    step_ad = 10  # adam 二次下降\n"]},{"cell_type":"markdown","metadata":{"id":"6hHrNxGDAn_6"},"source":["# Load PreTraining data #"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZgcBsNOg7yhI"},"outputs":[],"source":["batch_size = len(train_labels)\n","trainall = normalize_data(train_images.T)\n","testall = normalize_data(test_images.T)\n","\n","label_train = train_labels.T\n","label_train = tf.convert_to_tensor(label_train, dtype=tf.float32)\n","trainall = tf.convert_to_tensor(trainall, dtype=tf.float32)  # batch_size * feat nums  -->  feat nums * batch_size\n","\n","label_test = test_labels.T\n","label_test = tf.convert_to_tensor(label_test, dtype=tf.float32)\n","testall = tf.convert_to_tensor(testall, dtype=tf.float32)       # batch_size * feat nums  -->  feat nums * batch_size\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pc_1w85BbxuF"},"outputs":[],"source":["def optimizer(parameters, pinter, X, Y, epoc):\n","    def tenf(koa):\n","        koa = tf.convert_to_tensor(koa, dtype = tf.float32)\n","        return koa\n","    def subuout(ain, biin, tau_in):\n","        # ain batchsize * 10, bin too Cheaked at January 26th\n","        uf = biin\n","        row, column = np.nonzero(ain)\n","        biin = biin.numpy()\n","        cin = tf.transpose(tf.ones((10, batch_size_true)) * tf.convert_to_tensor(biin[row, column], dtype = tf.float32))\n","        uin2 = tf.minimum(biin, cin)\n","        condition1 = tf.sqrt(tf.convert_to_tensor(tau_in, dtype = tf.float32)) * tf.linalg.norm(biin - uin2, ord=2, axis=1)  # axis 1 represents row\n","        T1 = (condition1 < 2)\n","        uf = uf.numpy()\n","        uf[T1,:] = uin2.numpy()[T1,:]\n","        # uf = uin2\n","        return tf.convert_to_tensor(uf.T, dtype = tf.float32)  # tf.transpose(uf)  #\n","\n","\n","    def subuin(ain, biin, tau_in):\n","        epi = 3.e-7\n","        uin = biin.numpy()\n","        condition1 = (biin > 0) & (ain < 0.5) & (ain < (1 - tau_in * biin * biin) / 2)\n","        condition2 = (biin < 0) & (ain > 0.5) & (ain > (1 + tau_in * biin * biin) / 2)\n","        uin[condition1] = 0\n","        uin[condition2] = epi\n","        return tf.convert_to_tensor(uin, dtype = tf.float32)\n","\n","    def subw(win, bin, ain, tau_in, gam_in, epoc, layer):\n","        # Win.shape n_{layer-1}, n_layer\n","        # A.shape n_{layer-1}, N\n","        # B.shape n_{layer}, N\n","        b = 2 * tau_in * tf.matmul(ain, tf.transpose(bin))\n","        A = 2 * tau_in * tf.matmul(ain, tf.transpose(ain)) + gam_in * tf.eye(len(b))\n","        d_W = tf.matmul(A, win) - b  # * gamma~~~~~~~~~~~~\n","        W_try = win - lr * d_W\n","        if layer > 1:\n","          if epoc < 200:\n","            W_norm2 = prox_l0(W_try, sp0, vec_yn=0)\n","            idx_T = np.nonzero(W_norm2)[0]  # W行模的非0指标集Tk\n","            win = win.numpy()\n","            win[idx_T] = W_try.numpy()[idx_T]\n","            if epoc > 5:\n","              idx_Tbar = np.argwhere(W_norm2 == 0)[:, 0]\n","              win[idx_Tbar] = 0\n","          else:\n","            W_norm2 = prox_l0(W_try, sp0, vec_yn=0)\n","            idx_T = np.nonzero(W_norm2)[0]  # W行模的非0指标集Tk\n","            idx_Tbar = np.argwhere(W_norm2 == 0)[:, 0]\n","            win = win.numpy()\n","            ain = ain.numpy()\n","            b2 =  2 * tau_in * tf.matmul(tf.matmul(ain[idx_T], tf.transpose(ain[idx_Tbar])), win[idx_Tbar])\n","            b = tenf(b.numpy()[idx_T]) - b2\n","          if epoc > 200:\n","            if epoc < 150:\n","              win[idx_Tbar] -= 1.e-2 * win[idx_Tbar]\n","            else:\n","              win[idx_Tbar] = 0\n","            A1 = tf.linalg.inv(2 * tau_in * tf.matmul(tenf(ain[idx_T]), tenf(tf.transpose(ain[idx_T]))) + gam_in * tf.eye(len(idx_T)))\n","            win[idx_T] = tf.matmul(A1, b)\n","        else:\n","          if epoc % 12 > -10:\n","            win = W_try\n","          else:\n","            win = tf.matmul(tf.linalg.inv(A), b)\n","          idx_T = np.nonzero(tf.ones((win.shape[0], 1)))[0]\n","        return tf.convert_to_tensor(win, dtype = tf.float32), idx_T  # tf.convert_to_tensor(, dtype = tf.float32)\n","\n","    def subv(vin, idx, ain, bin, cin, pa_in, tau_in):  # 显示解\n","        # print(len(idx), vin.shape)\n","        # vin: 1000x64;  ain:  1000[idx+1]x10;  bin:  10, 64;   cin: 1000[idx+1], 64,\n","        for j in range(1):\n","            b = 2 * tau_in * tf.matmul(ain, bin) + 2 * pa_in * cin\n","            # vin -= lr * (2 * tau_in * tf.matmul(tf.matmul(ain, tf.transpose(ain)), vin) - b)\n","            A1 = tf.linalg.inv(2 * tau_in * tf.matmul(ain, tf.transpose(ain)) + 2 * pa_in * tf.eye(len(b)))\n","            # vin = vin.numpy()\n","            vin = tf.matmul(A1, b)\n","        return vin  # tf.convert_to_tensor(vin, dtype = tf.float32)\n","\n","    for layer in range(3, 0, -1):\n","        if layer == 3:\n","            # Y 10*64;  W3 (1000, 10); V3 (10, 64);\n","            pinter['U' + str(layer)] = \\\n","                subuout(tf.transpose(Y), tf.transpose(tf.matmul(tf.transpose(parameters[\"W\" + str(layer)]), pinter['V' + str(layer - 1)])), tau[layer] * pa[layer])\n","            for ko in range(1):\n","              parameters['W' + str(layer)], parameters[\"idx\" + str(layer)] = \\\n","                  subw(parameters['W' + str(layer)], pinter['U' + str(layer)], pinter['V' + str(layer - 1)], tau[layer], gam, epoc, layer)\n","        else:\n","            pinter['V' + str(layer)] = subv(pinter['V' + str(layer)], parameters[\"idx\" + str(layer + 1)],\n","                                            parameters[\"W\" + str(layer + 1)], pinter['U' + str(layer + 1)],\n","                                            sgn(pinter['U' + str(layer)]), pa[layer], tau[layer+1])  # , parameters[\"idx\" + str(layer + 1)])\n","            pinter['U' + str(layer)] = subuin(pinter['V' + str(layer)],\n","                                              tf.matmul(tf.transpose(parameters[\"W\" + str(layer)]), pinter['V' + str(layer - 1)]),\n","                                              tau[layer] / pa[layer])\n","            for ko in range(1):\n","              parameters['W' + str(layer)], parameters[\"idx\" + str(layer)] = \\\n","                  subw(parameters['W' + str(layer)], pinter['U' + str(layer)], pinter['V' + str(layer - 1)], tau[layer], gam, epoc, layer)\n","\n","    return parameters, pinter"]},{"cell_type":"markdown","metadata":{"id":"pDMKOIPsYL_q"},"source":["# **3 Train 0/1 DNN by PBCD**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k9hLbeR8tlOv"},"outputs":[],"source":["batch_size_pre = 128\n","batch_size_true = 256\n","# sp0_pre = 1 - 0.7\n","# time_all = [27.27 25.86 24.24  23.97   23.52 23.17   22.82   22.79 22.34 22.27]\n","# acc_all = [9923 9925 9926 9927 9931  9932  9929  9934  9921 9874]\n","sp0 = 1 - 0.9\n","lr = 3.2 # 0.0072  # 6.e-2 ~ 6.e-1  turned at Feb 17th\n","ship = 2.e-2  # slippage for the next step\n","gam = 1.e-4  # turned at Feb 27th 6.e-2  --->  6.e-3   gamma for penalty norm2 \"W\"\n","best_acc = 0.99323\n","# parameters = load_parameters(os.path.join(directory, \"preparameter\"))\n","# parameters, pa, tau, _ = initialize_with_zeros(n_x, n_h1, n_h2, n_y)\n","pa = 1.e-5 * np.ones((4, 1))     # 每一层罚参数初始值\n","pa[3] = 1 / (2 * batch_size_true)  # 每一层罚参数初始值\n","tau = 1.e-6 * np.ones((4, 1))    # 每一层罚参数初始值\n","\n","train_all_time = 5\n","all_times = 0\n","history1_all = np.zeros((50, 5))\n","history2_all = np.zeros((50, 5))\n","super1_all = np.ones((5, 1))\n","super1_all[0] = 0  # current best parameter 1.e-4:\n","super1_all[1] = 3  # current best parameter 1.e-4:\n","super1_all[2] = 5  # current best parameter 1.e-4:\n","super1_all[3] = 11 # current best parameter 1.e-4:\n","super1_all[4] = 12  # current best parameter 1.e-4:\n","epochs = 40\n","low_acc = -9800\n","best_acc_of_all = 0  # 1.2\n","best_para1 = 0  # epi573 = 0.01\n","for all_times in range(train_all_time):\n","    history1 = np.zeros((50,1))\n","    history2 = np.zeros((50,1))\n","    tf.random.set_seed(3 + (6 * all_times-1))\n","    parameters, _, _, g_sum = initialize_with_zeros(n_x, n_h1, n_h2, n_y)\n","    prenum = 1\n","    # real_number = np.ceil(len(train_labels)/batch_size_pre)\n","    for i in range(prenum):\n","        # print('We are in the Ecope {}:'.format(i))\n","        # with tqdm(total = real_number) as pbar:\n","        #####  pritrain process  #####\n","        #####  ##### ##### ##### ##### #####\n","        train_data = mini_batchs_generator(train_images, train_labels, batch_size_pre)\n","        for step, (img_train, label_train1) in enumerate(train_data):\n","            real_batch_size = len(label_train1)\n","            label_train_pre = label_train1.T\n","            imgvector = normalize_data(img_train.T)  # 784 *\n","            label_train_pre = tf.convert_to_tensor(label_train_pre, dtype=tf.float32)\n","            feat_train = tf.convert_to_tensor(imgvector, dtype=tf.float32)\n","            output, cache = forward_propagation_pre(feat_train, parameters)\n","            parameters, g_sum = optimizer_adam(parameters, cache, feat_train, label_train_pre, g_sum,\n","                                                  epsilon, delta, rho_1, rho_2, step_ad)\n","    # aa25, _ = forward_propagation(trainall, parameters)\n","    # acctrain = (60000 - costloss(aa25, label_train))/60000\n","    # print(\"pretrain:\",acctrain)\n","    # # try:\n","    for i in range(epochs):\n","\n","        train_data = mini_batchs_generator(train_images, train_labels, batch_size_pre)\n","        for step, (img_train, label_train1) in enumerate(train_data):\n","            batch_size_true = len(label_train1)\n","            label_train_pre = label_train1.T\n","            imgvector = normalize_data(img_train.T)  # 784 *\n","            label_train_pre = tf.convert_to_tensor(label_train_pre, dtype=tf.float32)\n","            feat_train = tf.convert_to_tensor(imgvector, dtype=tf.float32)\n","            output, cache = forward_propagation(feat_train, parameters)\n","            parameters, cache = optimizer(parameters, cache, feat_train, label_train_pre, i)\n","        # if i % 1 == 0:\n","        # merit = penaltyloss(parameters, cache, trainall, label_train)\n","        # acc_train = costloss(aa21, label_train)\n","        # print(\"After\", merit[\"cost1\"], merit[\"cost2\"], merit[\"cost3\"], merit[\"cost4\"],\n","        #       merit[\"cost5\"], merit[\"costall\"])\n","        # if merit[\"cost1\"] < 0.7 * merit[\"cost5\"]:\n","        #   gam *= 0.5\n","        #   print(\"gam is bigger!!!\")\n","        aa25, _ = forward_propagation(trainall, parameters)\n","        acctrain = (60000 - costloss(aa25, label_train))/60000\n","        history1[i] = acctrain\n","        aa25, _ = forward_propagation(testall, parameters)\n","        acctest = (10000 - costloss(aa25, label_test))/10000\n","        history2[i] = acctest\n","        # if history2[0] > 0.9923:\n","        #   break\n","        print(\"The{} th epoch, trainacc:{}, testacc:{}\".format(i, acctrain, acctest))\n","        # if acc < low_acc or merit[\"costall\"] > 10000:\n","        #     print(\"Too low acc:{} or merit:{}.\".format(acc, merit[\"costall\"]))\n","        #     break\n","        # else:\n","        #     print('Famal train: The :{}-th epoch, current super1:{}. and the current accuracy in this step: {}'.format(\n","        #             i, super1, acc))\n","\n","\n","    # if max(history2) > best_acc and history2[0] < 0.9923:\n","    # print('Updating best seed in this epoch:{}'.format(super1))\n","    history1_all[:, all_times] = history1.reshape(50,)\n","    history2_all[:, all_times] = history2.reshape(50,)\n","    np.save(os.path.join(directory, \"result/history1_all.npy\"), history1_all)\n","    np.save(os.path.join(directory, \"result/history2_all.npy\"), history2_all)\n","    print(\"All time is{}.\".format(all_times))\n","    # else:\n","    #   print('Current acc in this seed:{}'.format(max(history2)))\n","    # if all_times == 5:\n","    #   break\n","    # if best_acc > best_acc_of_all:\n","    #     best_acc_of_all = best_acc\n","    #     best_para1 = super1\n","    #     print(\"========================================================\")\n","    #     print(\"=======================New accuracy=====================\")\n","    #     print(\"The current best acc of all is: {}, super parameter1 is: {},\".format(\n","    #         best_acc_of_all, best_para1))\n","    #     print(\"========================================================\")\n","    # except:\n","    #     print(\"There are some wrong things, but pass.\")\n","\n","    #     continue   # break\n","#======================\n","#======Save results===\n","needit = 0 # if need to save final results into final picture file\n","if needit: # had been finished\n","  historytrain_all = np.load(os.path.join(directory, \"result/history1_all.npy\"))\n","  historytest_all =  np.load(os.path.join(directory, \"result/history2_all.npy\"))\n","  directorybase = (\"/content/drive/MyDrive/PBCDcode/MNIST_minibatch728/data/activations/01network\")\n","  if not os.path.exists(directorybase):\n","    os.makedirs(directorybase)\n","  directory_1 = os.path.join(directorybase, \"historytrain_all\"+'.npy')\n","  directory_2 = os.path.join(directorybase, \"historytest_all\"+'.npy')\n","  np.save(directory_1, historytrain_all)\n","  np.save(directory_2, historytest_all)\n","# print(\"The final best acc of all is: {}, super parameter1 is: {}:\".format( best_acc_of_all, best_para1))"]},{"cell_type":"code","source":["needit = 1 # if need to save final results into final picture file\n","if needit: # had been finished\n","  historytrain_all = np.load(os.path.join(directory, \"result/history1_all.npy\"))\n","  historytest_all =  np.load(os.path.join(directory, \"result/history2_all.npy\"))\n","\n","  directorybase = (\"/content/drive/MyDrive/PBCDcode/MNIST_minibatch728/data/activations/01network\")\n","  if not os.path.exists(directorybase):\n","    os.makedirs(directorybase)\n","\n","  directory_1 = os.path.join(directorybase, \"historytrain_all\"+'.npy')\n","  directory_2 = os.path.join(directorybase, \"historytest_all\"+'.npy')\n","  np.save(directory_1, historytrain_all)\n","  np.save(directory_2, historytest_all)\n","# print(\"The final best acc of all is: {}, super parameter1 is: {}:\".format( best_acc_of_all, best_para1))"],"metadata":{"id":"cN6a1r9OgMGw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Others"],"metadata":{"id":"gDjDUOhmIIZE"}}],"metadata":{"colab":{"collapsed_sections":["LWK5HBMyk5Bm"],"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}