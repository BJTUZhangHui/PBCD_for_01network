{"cells":[{"cell_type":"markdown","metadata":{"id":"tT2VMZJXYhTw"},"source":["# **Connect the google drive**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3559,"status":"ok","timestamp":1692283337813,"user":{"displayName":"Zhang Hui","userId":"06808414998828253018"},"user_tz":-480},"id":"5L1Ah48Ll3Nu","outputId":"077c568c-ab1a-482a-b6b9-b8c504ae8c93"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","# drive.mount(\"/content/drive/\", force_remount=True)\n","\n","# 1. Version  : 1 2. Time     : 5th May\n","# 3. Author   : Hui Zhang\n","# 4. Function : PBCD method to solve 2 hidden layer 0/1 DNN on Cifar10Resnet18\n","# 5. Structure: 2 hidden layer with 1000 nodes in each layer\n","# 6. Relation paper: PBCD method for 0/1 DNN **"]},{"cell_type":"markdown","metadata":{"id":"CXfp0lN9YW3v"},"source":["# **Define relavant function and load data**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kh7W8cowv1-6"},"outputs":[],"source":["import numpy as np   # science calculate  instill numpy\n","import struct  # y???????\n","from sklearn.utils import shuffle  # instill scikit-learn/ scikit\n","import time\n","import os\n","import scipy\n","import torch as tc\n","import tensorflow as tf\n","import matplotlib.dates as mdates\n","import matplotlib.pyplot as plt\n","\n","from tqdm import tqdm, trange\n","import tensorflow.keras as keras\n","from tensorflow.keras import datasets\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.layers import Dense,Conv2D,Dropout,Flatten,Activation,BatchNormalization,AveragePooling2D,MaxPooling2D\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.model_selection import train_test_split\n","\n","# Standardized data in 0-1\n","def normalize_data(ima):\n","    a_max = np.max(ima)  # 图像矩阵的最大值\n","    a_min = np.min(ima)  # 图像矩阵的最小值\n","    for i in range(ima.shape[0]):  # 矩阵第二维的长度：如3*4矩阵这个就是3\n","        for j in range(ima.shape[1]):  # numpy.core.fromnumeric 中的函数 第一维的长度\n","            ima[i][j] = (ima[i][j] - a_min) / (a_max - a_min)\n","    return ima  # 返回是一个矩阵\n","\n","# predeal dataset\n","def create_model(directory):\n","    # chanDim = -1\n","    input_shape = (28, 28, 1)\n","    model_original = load_model(os.path.join(directory, \"prenetwork/99.4_CNN.h5\"))\n","\n","    model = Sequential()\n","    # Layer 1 Conv2D\n","    cnn_layer = Conv2D(filters = 32, kernel_size = 5, strides = 1, activation = 'relu', input_shape = input_shape)\n","    cnn_layer.trainable=False\n","    model.add(cnn_layer)\n","    cnn_layer2= Conv2D(filters = 32, kernel_size = 5, strides = 1, use_bias=False)\n","    cnn_layer2.trainable=False\n","    model.add(cnn_layer2)\n","    cnn_layer3=BatchNormalization()\n","    cnn_layer3.trainable=False\n","    model.add(cnn_layer3)\n","    model.add(Activation('relu'))\n","    model.add(MaxPooling2D(pool_size = 2, strides = 2))\n","    model.add(Dropout(0.25,name = 'representation')) ############## 需要这一层的输出\n","    # Layer 3 Conv2D\n","    cnn_layer4= Conv2D(filters = 64, kernel_size = 3, strides = 1, activation = 'relu')\n","    cnn_layer4.trainable=False\n","    model.add(cnn_layer4)\n","    cnn_layer5= Conv2D(filters = 64, kernel_size = 3, strides = 1, use_bias=False)\n","    cnn_layer5.trainable=False\n","    model.add(cnn_layer5)\n","    cnn_layer6= BatchNormalization()\n","    cnn_layer6.trainable=False\n","    model.add(cnn_layer6)\n","    # Layer 4 Pooling Layer\n","    model.add(Activation('relu'))\n","    model.add(MaxPooling2D(pool_size = 2, strides = 2))\n","    model.add(Dropout(0.25))\n","    model.add(Flatten(name='flatten'))\n","\n","\n","    model.add(Dense(500, use_bias=False))\n","    model.add(BatchNormalization())\n","    model.add(Activation('relu'))\n","    model.add(Dense(300, use_bias=False))\n","    model.add(BatchNormalization())\n","    model.add(Activation('relu'))\n","    model.add(Dense(100, use_bias=False))\n","    model.add(BatchNormalization())\n","    model.add(Activation('relu'))\n","    model.add(Dropout(0.25))\n","    model.add(Dense(20, use_bias=False))\n","    model.add(BatchNormalization())\n","    model.add(Activation('relu'))\n","\n","    model.add(Dense(10, activation='softmax'))\n","    model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n","    model.layers[0].set_weights(model_original.layers[0].get_weights())\n","    model.layers[1].set_weights(model_original.layers[1].get_weights())\n","    model.layers[2].set_weights(model_original.layers[2].get_weights())\n","    model.layers[6].set_weights(model_original.layers[6].get_weights())\n","    model.layers[7].set_weights(model_original.layers[7].get_weights())\n","    model.layers[8].set_weights(model_original.layers[8].get_weights())\n","    return model\n","\n","\n","# 初始化各个参数 n_x x的维数 n_h1：第一隐层节点数（靠近x的）, n_h2：第二隐层节点数, n_y： 输出y的维数\n","def initialize_with_zeros(n_x, n_h1, n_h2, n_y):\n","\n","    W1 = tf.random.uniform([n_x, n_h1], minval = -np.sqrt(6) / np.sqrt(n_x + n_h1), maxval = np.sqrt(6) / np.sqrt(n_h1 + n_x), dtype=tf.dtypes.float32, seed=None, name=None)\n","    W2 = tf.random.uniform([n_h1, n_h2], minval = -np.sqrt(6) / np.sqrt(n_h1 + n_h2), maxval = np.sqrt(6) / np.sqrt(n_h1 + n_h2), dtype=tf.dtypes.float32, seed=None, name=None)\n","    W3 = tf.random.uniform([n_h2, n_y], minval = -np.sqrt(6) / np.sqrt(n_y + n_h2), maxval = np.sqrt(6) / np.sqrt(n_y + n_h2), dtype=tf.dtypes.float32, seed=None, name=None)\n","    dW1_1 = tf.zeros((n_x, n_h1))\n","    dW1_2 = tf.zeros((n_x, n_h1))\n","    dW2_1 = tf.zeros((n_h1, n_h2))\n","    dW2_2 = tf.zeros((n_h1, n_h2))\n","    dW3_1 = tf.zeros((n_h2, n_y))\n","    dW3_2 = tf.zeros((n_h2, n_y))\n","\n","    g_sum = {\"dW1_1\": dW1_1,\n","             \"dW1_2\": dW1_2,\n","             \"dW2_1\": dW2_1,\n","             \"dW2_2\": dW2_2,\n","             \"dW3_1\": dW3_1,\n","             \"dW3_2\": dW3_2\n","             }\n","    parameters = {\"W1\": W1,\n","                  \"W2\": W2,\n","                  \"W3\": W3,\n","                  \"idx1\": np.nonzero(tf.ones((n_x, 1)) )[0],\n","                  \"idx2\": np.nonzero(tf.ones((n_h1, 1)) )[0],  # initial\n","                  \"idx3\": np.nonzero(tf.ones((n_h2, 1)) )[0]  # initial\n","                  }\n","    pa_in = 1.e-7 * np.ones((4, 1))  # 每一层罚参数初始值\n","    pa_in[3] = 1 / (2 * batch_size)  # 每一层罚参数初始值\n","    tau_in = 1.e-6 * np.ones((4, 1))  # 每一层罚参数初始值\n","    return parameters, pa_in, tau_in, g_sum\n","\n","\n","# 存储参数\n","def save_parameters(parameters, directory):\n","    for key, val in parameters.items():\n","        if not os.path.exists(os.path.join(directory, str(key) + '.npy')):\n","          os.makedirs(os.path.join(directory, str(key) + '.npy'))\n","        tf.save(os.path.join(directory, str(key) + '.npy'), val)\n","\n","\n","def costloss(V3, Y):\n","    cost = tf.square(tf.norm(V3 - Y)) / 2\n","    return cost\n","\n","def penaltyloss(P, C, X, Y):\n","    cost1 = pa[0] * tf.square( tf.norm(Y - hardmax(C[\"U3\"])) )\n","    cost2 = tau[3] * tf.square(tf.norm( C[\"U3\"] - tf.matmul(tf.transpose(P['W3']),  C[\"V2\"]))  )\n","    cost3 = pa[2] * tf.square( tf.norm(C[\"V2\"] - sgn(C[\"U2\"])) ) + tau[2] * tf.square( tf.norm(C[\"U2\"] - tf.matmul(tf.transpose(P['W2']), C[\"V1\"])) )\n","    cost4 =  pa[1] * tf.square( tf.norm(C[\"V1\"] - sgn(C[\"U1\"])) ) + tau[1] * tf.square( tf.norm(C[\"U1\"] - tf.matmul(tf.transpose(P['W1']), X)) )\n","    cost5 = gam * (tf.square( tf.norm(P['W3'])) + tf.square(tf.norm(P['W2'])) + tf.square(tf.norm(P['W1'])))\n","\n","    cost = {\"cost1\": cost1.numpy(),\n","             \"cost2\": cost2.numpy(),\n","             \"cost3\": cost3.numpy(),\n","             \"cost4\": cost4.numpy(),\n","             \"cost5\": cost5.numpy(),\n","             \"costall\": cost1.numpy() + cost2.numpy() + cost3.numpy()+cost4.numpy()+cost5.numpy()\n","             }\n","    return cost\n","\n","def sgn(x):\n","    s = tf.sign(x * tf.cast(x > 0, dtype=tf.float32))\n","    return s\n","\n","\n","def softmax(x):  # 返回第一个最大值的位置\n","    v = tf.argmax(x)\n","    return v\n","\n","def hardmax(x):  # 返回矩阵值 dimision: 10 * 64\n","    v1 = np.zeros(x.shape)\n","    v = np.argmax(x, 0)  # tf.argmax(a,1) 每一列最大值位置\n","    for i in range(x.shape[1]):\n","        v1[v[i], i] = 1\n","\n","    return tf.convert_to_tensor(v1, dtype=tf.float32)\n","\n","# 将原矩阵拉成784列但不知道多少行（-1）的矩阵（np.会自动计算配套维数）\n","def image2vector(image):\n","    v = np.reshape(image, [-1, n_x])\n","    return v.T\n","\n","\n","def mini_batchs_generator(inputs, targets, batch_size):\n","    inputs_data_size = len(inputs)\n","    targets_data_size = len(targets)\n","    # print(\"len(inputs):={}, size of inputs {}, size of targets{}\".format(inputs_data_size, inputs.shape, targets.shape))\n","    assert inputs_data_size == targets_data_size, \"The length of inputs({}) and targets({}) must be consistent\".format(\n","        inputs_data_size, targets_data_size)\n","\n","    shuffled_input, shuffled_target = shuffle(inputs, targets)  # 元素随机排序？\n","    mini_batches = [(shuffled_input[k: k + batch_size], shuffled_target[k: k + batch_size])\n","                    for k in range(0, inputs_data_size, batch_size)]  #\n","    for x, y in mini_batches:\n","        # print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n","        # print(x.shape)\n","        yield x, y  # yield = return的功能 + 未终止再使用直到循环完毕(实现一个epoch功能)\n","\n","def produce_step_adam(grad_1, grad_2, g_new, epsilon, delta, rho_1, rho_2, step_ad):\n","    \"\"\"\n","    adam\n","     (i) 梯度滑动- 一阶矩  g = rho_1 * grad_1 + (1-rho_1)*g_new\n","     (ii) 二阶矩  r_t = rho_2 * grad + (1-rho_1)*g_new*g_new\n","     (iii) 更新  step = epsilon /sqrt(delta+r_t)\n","    \"\"\"\n","    grad_1 = rho_1 * grad_1 + (1 - rho_1) * g_new\n","    g = grad_1 / (1 - pow(rho_1, step_ad))\n","    r_t = rho_2 * grad_2 + (1 - rho_2) * g_new * g_new\n","    r = r_t / (1 - pow(rho_2, step_ad))\n","    step = epsilon / tf.sqrt(delta + r)\n","    return step, g, grad_1, r_t\n","\n","def forward_propagation_pre(x_tol, parameters_for):\n","    layers_num = 3\n","    cache_for = {}\n","    layer_input = x_tol\n","    for layer in range(1, layers_num + 1):\n","        locals()['A' + str(layer)] = tf.matmul(tf.transpose(parameters_for['W' + str(layer)]),\n","                                            layer_input)\n","        cache_for['A' + str(layer)] = locals()['A' + str(layer)]\n","        locals()['Z' + str(layer)] = locals()['A' + str(layer)] * tf.cast(locals()['A' + str(layer)] > 0, dtype=tf.float32)\n","        layer_input = locals()['Z' + str(layer)]\n","        cache_for['Z' + str(layer)] = locals()['Z' + str(layer)]\n","    return locals()['Z' + str(layers_num)], cache_for\n","\n","def prox_l0(x, sparse, vec_yn):\n","    k = int(x.shape[0] * sp0)\n","    abx = np.linalg.norm(x, axis=1, keepdims=True)\n","    # print(abx.shape, x.shape[0], k)\n","    thre = np.partition(abx, kth=k, axis=None)[k]\n","    abx *= (abx >= thre)\n","    # print(np.partition(x, kth=k, axis=None)[k], np.sqrt(2 * 0.13 * sparse * lr))\n","    # x *= x >= np.sqrt(2 * sparse * lr)\n","    # if k > 5:\n","    #     thre = np.partition(x, kth=k, axis=None)[k]\n","    #     x *= (x >= thre)\n","    return abx\n","\n","def optimizer_adam(parameters, cache, X, Y, g_sum, epsilon, delta, rho_1, rho_2, step_ad, iht=False):\n","    A1 = cache[\"Z1\"]\n","    A2 = cache[\"Z2\"]\n","    A3 = cache[\"Z3\"]\n","\n","\n","    dZ3 = (A3 - Y)  # * Z3(1-Z3)  # h2 * batch\n","    dZ2 = tf.matmul(parameters[\"W3\"], dZ3) * tf.cast(A2 > 0,dtype=tf.float32)\n","    dZ1 = tf.matmul(parameters[\"W2\"], dZ2) * tf.cast(A1 > 0,dtype=tf.float32)\n","\n","    dW3 = tf.matmul((A2), tf.transpose(dZ3))\n","    dW2 = tf.matmul((A1), tf.transpose(dZ2))\n","    dW1 = tf.matmul((X), tf.transpose(dZ1))\n","\n","    step_W3, dW3, g_sum[\"dW3_1\"], g_sum[\"dW3_2\"] = produce_step_adam(g_sum[\"dW3_1\"], g_sum[\"dW3_2\"], dW3,\n","                                                                     epsilon, delta, rho_1, rho_2, step_ad)\n","    step_W2, dW2, g_sum[\"dW2_1\"], g_sum[\"dW2_2\"] = produce_step_adam(g_sum[\"dW2_1\"], g_sum[\"dW2_2\"], dW2,\n","                                                                     epsilon, delta, rho_1, rho_2, step_ad)\n","    step_W1, dW1, g_sum[\"dW1_1\"], g_sum[\"dW1_2\"] = produce_step_adam(g_sum[\"dW1_1\"], g_sum[\"dW1_2\"], dW1, epsilon,\n","                                                                     delta, rho_1, rho_2, step_ad)\n","    parameters[\"W1\"] -=  step_W1 * dW1\n","    ep = 1\n","    if ep > -10:\n","      parameters[\"W3\"] -=  step_W3 * dW3\n","      parameters[\"W2\"] -=  step_W2 * dW2\n","    else:\n","      sp0_pre = 0.7\n","      w3 = parameters[\"W3\"].numpy()\n","      w2 = parameters[\"W2\"].numpy()\n","      ep3 = 0 * w3\n","      ep2 = 0 * w2\n","\n","      W_norm2 = prox_l0(w3, sp0_pre, vec_yn=0)\n","      idx_T = np.nonzero(W_norm2)[0]  # W行模的非0指标集Tk\n","      ep3[idx_T] = w3[idx_T] - step_W3.numpy()[idx_T] * dW3.numpy()[idx_T]\n","\n","      W_norm2 = prox_l0(w2, sp0_pre, vec_yn=0)\n","      idx_T = np.nonzero(W_norm2)[0]  # W行模的非0指标集Tk\n","      ep2[idx_T] = w2[idx_T] - step_W2.numpy()[idx_T] * dW2.numpy()[idx_T]\n","      parameters[\"W2\"] = tf.convert_to_tensor(w2, dtype = tf.float32)\n","      parameters[\"W3\"] = tf.convert_to_tensor(w3, dtype = tf.float32)\n","\n","    return parameters, g_sum\n","\n","\n","# feed forward\n","def forward_propagation(X, parameters):\n","    W1 = parameters[\"W1\"]  # W1 维数 （784, 1000）\n","    W2 = parameters[\"W2\"]\n","    W3 = parameters[\"W3\"]\n","\n","    U1 = tf.matmul(tf.transpose(W1), X)  # + b1 * parameters['gamma1']\n","    # betch_size =16, nx*nx = 784 X为（784，16）, gamma1为nx*nx，X * gamma1的维数和X维数相同\n","    V1 = sgn(U1)  # 实现Relu，V1为 (1000，16)为第1隐层的值\n","    U2 = tf.matmul(tf.transpose(W2), V1)  # + b2    # W2^T 维数 （1000,1000） * parameters['gamma2']\n","    V2 = sgn(U2)  # 实现Relu，V2 (1000，16)为第2隐层的值\n","    U3 = tf.matmul(tf.transpose(W3), V2)  # + b3    # W3^T为 （10，1000） * parameters['gamma3']\n","    V3 = hardmax(U3)  # 输出 维数是10*batch 大小\n","\n","    cache = {\n","        \"V0\": X,\n","        \"U0\": X,\n","        \"U1\": U1,\n","        \"V1\": V1,\n","        \"U2\": U2,\n","        \"V2\": V2,\n","        \"U3\": U3,\n","        \"V3\": V3}\n","\n","    return V3, cache\n","\n","def save_parameters(parameters, directoryin):\n","    for key, val in parameters.items():\n","        np.save(os.path.join(directoryin, str(key)+'.npy'), val)\n","\n","def load_parameters(directoryin):\n","    W1 = np.load(os.path.join(directoryin, 'W1.npy'))\n","    W2 = np.load(os.path.join(directoryin, 'W2.npy'))\n","    W3 = np.load(os.path.join(directoryin, 'W3.npy'))\n","    parameters = {\"idx1\": np.nonzero(np.ones((W1.shape[0], 1)))[0],\n","                  \"idx2\": np.nonzero(np.ones((W2.shape[0], 1)))[0],\n","                  \"idx3\": np.nonzero(np.ones((W3.shape[0], 1)))[0],\n","                  \"W1\": tf.convert_to_tensor(W1, dtype=tf.float32),\n","                  \"W2\": tf.convert_to_tensor(W2, dtype=tf.float32),\n","                  \"W3\": tf.convert_to_tensor(W3, dtype=tf.float32)\n","                  }\n","    return parameters\n","\n","if __name__ == '__main__':\n","    data_name = \"Cifar1010\"\n","    path_dict = {1: \"01_BCD\"}\n","    directory = (\"/content/drive/MyDrive/PBCDcode/Cifar10Resnet18/data/\")\n","    if not os.path.exists(directory):\n","        os.makedirs(directory)\n"]},{"cell_type":"markdown","metadata":{"id":"oUxdmTzZ0uhW"},"source":["# Load dataset#"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1096,"status":"ok","timestamp":1692283340697,"user":{"displayName":"Zhang Hui","userId":"06808414998828253018"},"user_tz":-480},"id":"lo54UyYqiC7L","outputId":"a66a425c-1361-4f1d-ffd0-81b8c2c1eb96"},"outputs":[{"name":"stdout","output_type":"stream","text":["The right shape is samplesize times featuresize: (50000, 512)\n"]}],"source":["train_images = np.load(os.path.join(directory, \"rawdata/train_images.npy\"))\n","test_images = np.load(os.path.join(directory, \"rawdata/test_images.npy\"))\n","train_labels = np.load(os.path.join(directory, \"rawdata/train_labels.npy\"))\n","test_labels = np.load(os.path.join(directory, \"rawdata/test_labels.npy\"))\n","\n","# normal parameters\n","if 1:\n","    print(\"The right shape is samplesize times featuresize:\",train_images.shape)\n","    n_x = train_images.shape[1]  # 764--> 578\n","    # n_h1 = 1000  # 第一隐层 Will changed later\n","    # n_h2 = 1000  # 第二隐层 Will changed later\n","    n_y = 10  # 输出层 Out put layer\n","    # adam parameters\n","    epsilon = 1.e-3  # adam步长的分子\n","    delta = 1.e-8  # adam步长的第一个分母\n","    rho_1 = 0.9  # adam 一阶距滑动系数\n","    rho_2 = 0.99  # adam 二阶距滑动系数\n","    step_ad = 10  # adam 二次下降\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1692283340698,"user":{"displayName":"Zhang Hui","userId":"06808414998828253018"},"user_tz":-480},"id":"tNvDEPYBUipi","outputId":"efd22eac-6565-4765-e283-6550e3caa465"},"outputs":[{"name":"stdout","output_type":"stream","text":["(10000, 10)\n"]}],"source":["print(test_labels.shape)"]},{"cell_type":"markdown","metadata":{"id":"6hHrNxGDAn_6"},"source":["# Predeal data #"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29991,"status":"ok","timestamp":1692283371820,"user":{"displayName":"Zhang Hui","userId":"06808414998828253018"},"user_tz":-480},"id":"ZgcBsNOg7yhI","outputId":"aecf1f02-8a80-4cd7-e2ba-4267f488f2ba"},"outputs":[{"name":"stdout","output_type":"stream","text":["the right size is feat nums * sample_size (512, 10000)\n"]}],"source":["batch_size = len(train_labels)\n","trainall = normalize_data(train_images.T)\n","testall = normalize_data(test_images.T)\n","\n","label_train = train_labels.T\n","label_train = tf.convert_to_tensor(label_train, dtype=tf.float32)\n","trainall = tf.convert_to_tensor(trainall, dtype=tf.float32)  # batch_size * feat nums  -->  feat nums * batch_size\n","\n","label_test = test_labels.T\n","label_test = tf.convert_to_tensor(label_test, dtype=tf.float32)\n","testall = tf.convert_to_tensor(testall, dtype=tf.float32)       # batch_size * feat nums  -->  feat nums * batch_size\n","print(\"the right size is feat nums * sample_size\", testall.shape)\n"]},{"cell_type":"markdown","metadata":{"id":"W_6tSL9AhFNr"},"source":["# **Define optimizer**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pc_1w85BbxuF"},"outputs":[],"source":["def optimizer(parameters, pinter, X, Y, epoc, batch_size_now):\n","  def tenf(koa):\n","      koa = tf.convert_to_tensor(koa, dtype = tf.float32)\n","      return koa\n","  def subuout(ain, biin, tau_in):\n","      # ain batchsize * 10, bin too Cheaked at January 26th\n","      uf = biin\n","      row, column = np.nonzero(ain)\n","      biin = biin.numpy()\n","      cin = tf.transpose(tf.ones((10, batch_size_now)) * tf.convert_to_tensor(biin[row, column], dtype = tf.float32))\n","      uin2 = tf.minimum(biin, cin)\n","      condition1 = tf.sqrt(tf.convert_to_tensor(tau_in, dtype = tf.float32)) * tf.linalg.norm(biin - uin2, ord=2, axis=1)  # axis 1 represents row\n","      T1 = (condition1 < 2)\n","      uf = uf.numpy()\n","      uf[T1,:] = uin2.numpy()[T1,:]\n","      # uf = uin2\n","      return tf.convert_to_tensor(uf.T, dtype = tf.float32)  # tf.transpose(uf)  #\n","\n","\n","  def subuin(ain, biin, tau_in):\n","      epi = 3.e-5\n","      uin = biin.numpy()\n","      condition1 = (biin > 0) & (ain < 0.5) & (ain < (1 - tau_in * biin * biin) / 2)\n","      condition2 = (biin < 0) & (ain > 0.5) & (ain > (1 + tau_in * biin * biin) / 2)\n","      uin[condition1] = 0\n","      uin[condition2] = epi\n","      return tf.convert_to_tensor(uin, dtype = tf.float32)\n","\n","  def subw(win, bin, ain, tau_in, gam_in, epoc, layer):\n","      # Win.shape n_{layer-1}, n_layer\n","      # A.shape n_{layer-1}, N\n","      # B.shape n_{layer}, N\n","      b = 2 * tau_in * tf.matmul(ain, tf.transpose(bin))\n","      A = 2 * tau_in * tf.matmul(ain, tf.transpose(ain)) + gam_in * tf.eye(len(b))\n","      d_W = tf.matmul(A, win) - b  # * gamma~~~~~~~~~~~~\n","      W_try = win - lr * d_W\n","      if layer > 1:\n","        if epoc < 200:\n","          W_norm2 = prox_l0(W_try, sp0, vec_yn=0)\n","          idx_T = np.nonzero(W_norm2)[0]  # W行模的非0指标集Tk\n","          win = win.numpy()\n","          win[idx_T] = W_try.numpy()[idx_T]\n","          if epoc > 13:\n","            idx_Tbar = np.argwhere(W_norm2 == 0)[:, 0]\n","            win[idx_Tbar] = 0\n","        else:\n","          W_norm2 = prox_l0(W_try, sp0, vec_yn=0)\n","          idx_T = np.nonzero(W_norm2)[0]  # W行模的非0指标集Tk\n","          idx_Tbar = np.argwhere(W_norm2 == 0)[:, 0]\n","          win = win.numpy()\n","          ain = ain.numpy()\n","          b2 =  2 * tau_in * tf.matmul(tf.matmul(ain[idx_T], tf.transpose(ain[idx_Tbar])), win[idx_Tbar])\n","          b = tenf(b.numpy()[idx_T]) - b2\n","        if epoc > 200:\n","          if epoc < 150:\n","            win[idx_Tbar] -= 3.e-1 * win[idx_Tbar]\n","          else:\n","            win[idx_Tbar] = 0\n","          A1 = tf.linalg.inv(2 * tau_in * tf.matmul(tenf(ain[idx_T]), tenf(tf.transpose(ain[idx_T]))) + gam_in * tf.eye(len(idx_T)))\n","          win[idx_T] = tf.matmul(A1, b)\n","      else:\n","        if epoc % 12 > -10:\n","          win = W_try\n","        else:\n","          win = tf.matmul(tf.linalg.inv(A), b)\n","        idx_T = np.nonzero(tf.ones((win.shape[0], 1)))[0]\n","      return tf.convert_to_tensor(win, dtype = tf.float32), idx_T  # tf.convert_to_tensor(, dtype = tf.float32)\n","\n","\n","  def conjugate_gradient(A, b, x0, tol=1e-3, max_iter=40):\n","      x = x0.copy()\n","      r = b - A.dot(x)\n","      p = r.copy()\n","      r_dot_r = np.linalg.norm(p, 'fro')\n","      r_dot_r *= r_dot_r\n","\n","      if max_iter is None:\n","          max_iter = len(b)\n","\n","      for i in range(max_iter):\n","          Ap = A.dot(p)\n","          alpha = r_dot_r / np.sum(np.sum(np.multiply(p, Ap)))\n","          x += alpha * p\n","          r -= alpha * Ap\n","          r_dot_r_new = np.linalg.norm(r, 'fro')\n","          r_dot_r_new *= r_dot_r_new\n","\n","          if np.sqrt(r_dot_r_new) < tol:\n","              break\n","\n","          p = r + (r_dot_r_new / r_dot_r) * p\n","          r_dot_r = r_dot_r_new\n","\n","      return tf.convert_to_tensor(x, dtype = tf.float32)\n","\n","\n","  def subv(vin, idx, ain, bin, cin, pa_in, tau_in):  # 显示解\n","      # print(len(idx), vin.shape)\n","      # vin: 1000x64;  ain:  1000[idx+1]x10;  bin:  10, 64;   cin: 1000[idx+1], 64,\n","      for j in range(1):\n","          b = 2 * tau_in * tf.matmul(ain, bin) + 2 * pa_in * cin\n","          # vin -= lr * (2 * tau_in * tf.matmul(tf.matmul(ain, tf.transpose(ain)), vin) - b)\n","          A1 = 2 * tau_in * tf.matmul(ain, tf.transpose(ain)) + 2 * pa_in * tf.eye(b.shape[0])\n","          x0 = tf.random.uniform(vin.shape)\n","          vin =  conjugate_gradient(A1.numpy(), b.numpy(), x0.numpy())\n","          # vin = tf.matmul(A1, b)\n","      return vin  # tf.convert_to_tensor(vin, dtype = tf.float32)\n","\n","  def subwout(w_now, v_now, y_predict, wout_step):\n","      # ain batchsize * 10, bin too Cheaked at January 26th\n","      dz = tf.matmul(tf.transpose(w_now), v_now) - y_predict\n","      w_now -= wout_step * tf.matmul(v_now, tf.transpose(dz))\n","      v_now -= wout_step * tf.matmul(w_now, dz)\n","\n","\n","      if epoc < 200:\n","        w_ind = w_now.numpy()\n","        W_norm2 = prox_l0(w_ind, sp0, vec_yn=0)\n","        idx_T = np.nonzero(W_norm2)[0]  # W行模的非0指标集Tk\n","        # win = win.numpy()\n","        # win[idx_T] = w_now.numpy()[idx_T]\n","        # if epoc > 5:\n","        #   idx_Tbar = np.argwhere(W_norm2 == 0)[:, 0]\n","        #   win[idx_Tbar] = 0\n","\n","      return w_now, v_now, idx_T  # tf.transpose(uf)  #\n","\n","  if flag_output: # 1 means softmax, 0 means hardmax\n","    for layer in range(3, 0, -1):\n","        if layer == 3:\n","            # Y 10*64;  W3 (1000, 10); V3 (10, 64);\n","            for ko in range(1):\n","              parameters['W' + str(layer)], pinter['V' + str(layer - 1)], parameters[\"idx\" + str(layer)] = \\\n","                  subwout(parameters['W' + str(layer)], pinter['V' + str(layer - 1)], Y, wout_step)\n","        else:\n","            if layer == 1:\n","              pinter['V' + str(layer)] = subv(pinter['V' + str(layer)], parameters[\"idx\" + str(layer + 1)],\n","                                              parameters[\"W\" + str(layer + 1)], pinter['U' + str(layer + 1)],\n","                                              sgn(pinter['U' + str(layer)]), pa[layer], tau[layer+1])  # , parameters[\"idx\" + str(layer + 1)])\n","\n","            pinter['U' + str(layer)] = subuin(pinter['V' + str(layer)], tf.matmul(tf.transpose(parameters[\"W\" + str(layer)]), pinter['V' + str(layer - 1)]), tau[layer] / pa[layer])\n","\n","            for ko in range(1):\n","              parameters['W' + str(layer)], parameters[\"idx\" + str(layer)] = \\\n","                      subw(parameters['W' + str(layer)], pinter['U' + str(layer)], pinter['V' + str(layer - 1)], tau[layer], gam, epoc, layer)\n","\n","\n","  else:\n","    for layer in range(3, 0, -1):\n","        if layer == 3:\n","            # Y 10*64;  W3 (1000, 10); V3 (10, 64);\n","            pinter['U' + str(layer)] = \\\n","                subuout(tf.transpose(Y), tf.transpose(tf.matmul(tf.transpose(parameters[\"W\" + str(layer)]), pinter['V' + str(layer - 1)])), tau[layer] * pa[layer])\n","            for ko in range(1):\n","              parameters['W' + str(layer)], parameters[\"idx\" + str(layer)] = \\\n","                  subw(parameters['W' + str(layer)], pinter['U' + str(layer)], pinter['V' + str(layer - 1)], tau[layer], gam, epoc, layer)\n","        else:\n","            pinter['V' + str(layer)] = subv(pinter['V' + str(layer)], parameters[\"idx\" + str(layer + 1)],\n","                                            parameters[\"W\" + str(layer + 1)], pinter['U' + str(layer + 1)],\n","                                            sgn(pinter['U' + str(layer)]), pa[layer], tau[layer+1])  # , parameters[\"idx\" + str(layer + 1)])\n","            for ko in range(1):\n","              parameters['W' + str(layer)], parameters[\"idx\" + str(layer)] = \\\n","                  subw(parameters['W' + str(layer)], pinter['U' + str(layer)], pinter['V' + str(layer - 1)], tau[layer], gam, epoc, layer)\n","            pinter['U' + str(layer)] = subuin(pinter['V' + str(layer)],\n","                                              tf.matmul(tf.transpose(parameters[\"W\" + str(layer)]), pinter['V' + str(layer - 1)]),\n","                                              tau[layer] / pa[layer])\n","  return parameters, pinter"]},{"cell_type":"markdown","metadata":{"id":"pDMKOIPsYL_q"},"source":["# **3 Train 0/1 DNN by PBCD**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"k9hLbeR8tlOv","outputId":"c65d117e-1127-42c1-b6f8-b58e4ca2d340"},"outputs":[{"name":"stdout","output_type":"stream","text":["========================================================================================\n","The current super parameter is 1e-05.\n","The :0-th epoch, current super1:1e-05. and accuracy: 0.9735166430473328; 0.8123000264167786.\n","The :1-th epoch, current super1:1e-05. and accuracy: 0.9736999869346619; 0.8125.\n","The :2-th epoch, current super1:1e-05. and accuracy: 0.9738166928291321; 0.8126999735832214.\n","The :3-th epoch, current super1:1e-05. and accuracy: 0.9740166664123535; 0.8130999803543091.\n","The :4-th epoch, current super1:1e-05. and accuracy: 0.9740999937057495; 0.8133000135421753.\n","The :5-th epoch, current super1:1e-05. and accuracy: 0.974133312702179; 0.8133999705314636.\n","The :6-th epoch, current super1:1e-05. and accuracy: 0.9741666913032532; 0.8137000203132629.\n","The :7-th epoch, current super1:1e-05. and accuracy: 0.9743499755859375; 0.8136000037193298.\n","The :8-th epoch, current super1:1e-05. and accuracy: 0.974566638469696; 0.8137000203132629.\n","The :9-th epoch, current super1:1e-05. and accuracy: 0.9746166467666626; 0.8140000104904175.\n","The :10-th epoch, current super1:1e-05. and accuracy: 0.9746666550636292; 0.8144999742507935.\n","The :11-th epoch, current super1:1e-05. and accuracy: 0.9747666716575623; 0.8145999908447266.\n","The :12-th epoch, current super1:1e-05. and accuracy: 0.9749500155448914; 0.8148999810218811.\n","The :13-th epoch, current super1:1e-05. and accuracy: 0.9750499725341797; 0.8155999779701233.\n","The :14-th epoch, current super1:1e-05. and accuracy: 0.974566638469696; 0.8151000142097473.\n","The :15-th epoch, current super1:1e-05. and accuracy: 0.9746500253677368; 0.8152999877929688.\n","The :16-th epoch, current super1:1e-05. and accuracy: 0.9747499823570251; 0.8155999779701233.\n","The :17-th epoch, current super1:1e-05. and accuracy: 0.9747999906539917; 0.815500020980835.\n","The :18-th epoch, current super1:1e-05. and accuracy: 0.9748499989509583; 0.8158000111579895.\n","The :19-th epoch, current super1:1e-05. and accuracy: 0.9748499989509583; 0.8154000043869019.\n","The :20-th epoch, current super1:1e-05. and accuracy: 0.9749166369438171; 0.816100001335144.\n","The :21-th epoch, current super1:1e-05. and accuracy: 0.9749666452407837; 0.8159999847412109.\n","The :22-th epoch, current super1:1e-05. and accuracy: 0.9750166535377502; 0.816100001335144.\n"]}],"source":["batch_size = 256 # len(train_labels)\n","batch_size_pre = 256\n","n_h1 = 2000  # 第一隐层\n","n_h2 = 1000  # 第二隐层\n","# sp0_pre = 1 - 0.7\n","# time_all = [27.27 25.86 24.24  23.97   23.52 23.17   22.82   22.79 22.34 22.27]\n","# acc_all = [9923 9925 9926 9927 9931  9932  9929  9934  9921 9874]\n","sp0 = 1 - 0.78  #\n","lr = 5.e-1  # 6.e-2 ~ 6.e-1  turned at Feb 17th\n","ship = 2.e-2  # slippage for the next step\n","gam = 1.e-8  # turned at Feb 27th 6.e-2  --->  6.e-3   gamma for penalty norm2 \"W\"\n","# parameters = load_parameters(os.path.join(directory, \"preparameter\"))\n","# parameters, pa, tau, _ = initialize_with_zeros(n_x, n_h1, n_h2, n_y)\n","pa = 1.e-5 * np.ones((4, 1))  # 每一层罚参数初始值\n","pa[3] = 1 / (2 * batch_size)  # 每一层罚参数初始值\n","tau = 1.e-6 * np.ones((4, 1))  # [1.e-5, 1.e-7]\n","\n","train_all_time = [ 1.e-8 ] #  1.e-4, 1.e-6, 1.e-7, 1.e-8]\n","all_times = 0\n","history1_all = np.zeros((50, 5))\n","history2_all = np.zeros((50, 5))\n","# super1_all = np.ones((5, 1))\n","# super1_all[0] = 0   # current best parameter 1.e-4:\n","# super1_all[1] = 3   # current best parameter 1.e-4:\n","# super1_all[2] = 5   # current best parameter 1.e-4:\n","# super1_all[3] = 11  # current best parameter 1.e-4:\n","# super1_all[4] = 12  # current best parameter 1.e-4:\n","epochs = 40\n","low_acc = 0.8\n","best_acc_of_all = 0  # 1.2\n","best_para1 = 0    # epi573 = 0.01\n","tf.random.set_seed(21)\n","super1 = 0\n","\n","flag_output = 0\n","wout_step = 1.e-7\n","\n","# while all_times < 5:\n","for super1 in train_all_time:\n","  best_acc = -1\n","  wout_step = super1\n","  print(\"========================================================================================\")\n","  print(\"The current super parameter is {}.\".format(super1))\n","  # history1 = np.zeros((50,1))\n","  # history2 = np.zeros((50,1))\n","\n","  directorpre = os.path.join(directory, \"preparameter\")\n","  #========================================================================================\n","  #================================ To turn the parameter ============================\n","  parameters = load_parameters(directorpre)\n","\n","  #========================================================================================\n","  #============================= Pretrain once can save ==============================\n","  # parameters, _, _, g_sum = initialize_with_zeros(n_x, n_h1, n_h2, n_y)\n","  # prenum = 2\n","  # # real_number = np.ceil(len(train_labels)/batch_size_pre)\n","  # for i in range(prenum):\n","  #     # print('We are in the Ecope {}:'.format(i))\n","  #     # with tqdm(total = real_number) as pbar:\n","  #     #####  pritrain process  #####\n","  #     #####  ##### ##### ##### ##### #####\n","  #     train_data = mini_batchs_generator(train_images, train_labels, batch_size_pre)\n","  #     for step, (img_train, label_train1) in enumerate(train_data):\n","  #         real_batch_size = len(label_train1)\n","  #         label_train_pre = label_train1.T\n","  #         imgvector = normalize_data(img_train.T)  # 784 *\n","  #         label_train_pre = tf.convert_to_tensor(label_train_pre, dtype=tf.float32)\n","  #         feat_train = tf.convert_to_tensor(imgvector, dtype=tf.float32)\n","  #         output, cache = forward_propagation_pre(feat_train, parameters)\n","  #         parameters, g_sum = optimizer_adam(parameters, cache, feat_train, label_train_pre, g_sum,\n","  #                                               epsilon, delta, rho_1, rho_2, step_ad)\n","  # aa25, _ = forward_propagation(trainall, parameters)\n","  # acctrain = (60000 - costloss(aa25, label_train))/60000\n","  # print(\"pretrain:\",acctrain)\n","  # save_parameters(parameters, directorpre)\n","  #============================= Pretrain once can save ==============================\n","  #========================================================================================\n","  # try:\n","  for i in range(epochs):\n","      # if i % 4 == 0 or i > 20:   # decide when to update the cache\n","      #   aa21, cache = forward_propagation(trainall, parameters)\n","      # t0 = time.time()\n","      train_data = mini_batchs_generator(train_images, train_labels, batch_size_pre)\n","      for step, (img_train, label_train1) in enumerate(train_data):\n","        batch_size_true = len(label_train1)\n","        label_train_bat = label_train1.T\n","        imgvector = normalize_data(img_train.T)  # 784 *\n","        label_train_bat = tf.convert_to_tensor(label_train_bat, dtype=tf.float32)\n","        image_train = tf.convert_to_tensor(imgvector, dtype=tf.float32)\n","        output, cache = forward_propagation(image_train, parameters)\n","        for oin in range(1):\n","          parameters, cache = optimizer(parameters, cache, image_train, label_train_bat, i, batch_size_true)\n","\n","      # t1 = time.time()\n","      # if i % 1 == 0:\n","      # merit = penaltyloss(parameters, cache, trainall, label_train)\n","      # acc_train = costloss(aa21, label_train)\n","      # print(\"After\", merit[\"cost1\"], merit[\"cost2\"], merit[\"cost3\"], merit[\"cost4\"],\n","      #       merit[\"cost5\"], merit[\"costall\"])\n","      # if merit[\"cost1\"] < 0.7 * merit[\"cost5\"]:\n","      #   gam *= 0.5\n","      #   print(\"gam is bigger!!!\")\n","      aa25, _ = forward_propagation(trainall, parameters)\n","      acc_train = (60000 - costloss(aa25, label_train))/60000\n","\n","      if acc_train > best_acc:\n","        best_acc = acc_train\n","\n","      # history1[i] = acc_train\n","      aa25, _ = forward_propagation(testall, parameters)\n","      acc_test = (10000 - costloss(aa25, label_test))/10000\n","      # history2[i] = acc_test\n","      # if history2[0] > 0.9923:\n","      #   break\n","      # print(\"The{} th epoch, trainacc:{}, testacc:{}\".format(i, acctrain, acctest))\n","      if acc_test < low_acc:\n","        print(acc_test, low_acc)\n","        print(\"Too low acc:{}.\".format(acc_test))\n","        break\n","      else:\n","        print('The :{}-th epoch, current super1:{}. and accuracy: {}; {}.'.format(\n","                  i, super1, acc_train, acc_test))\n","  # if max(history2) > best_acc and history2[0] < 0.9923:\n","  # print('Updating best seed in this epoch:{}'.format(super1))\n","  # history1_all[:, all_times] = history1.reshape(50,)\n","  # history2_all[:, all_times] = history2.reshape(50,)\n","  # # np.save(os.path.join(directory, \"result/history1_all.npy\"), history1_all)\n","  # # np.save(os.path.join(directory, \"result/history2_all.npy\"), history2_all)\n","  # all_times += 1\n","  # if all_times == 5:\n","  #   break\n","  # print(\"All time is{}.\".format(all_times))\n","  # else:\n","  #   print('Current acc in this seed:{}'.format(max(history2)))\n","  # if all_times == 5:\n","  #   break\n","  if best_acc > best_acc_of_all:\n","      best_acc_of_all = best_acc\n","      best_para1 = super1\n","      print(\"========================================================\")\n","      print(\"=======================New accuracy=====================\")\n","      print(\"The current best acc of all is: {}, super parameter1 is: {},\".format(\n","          best_acc_of_all, best_para1))\n","      print(\"========================================================\")\n","  # except:\n","  #   print(\"There are some wrong things, but pass.\")\n","  #   continue   # break\n","#==================================================================================\n","#============================ Save results =====================================\n","# if needit: # had been finished\n","# historytrain_all = np.load(os.path.join(directory, \"result/history1_all.npy\"))\n","# historytest_all =  np.load(os.path.join(directory, \"result/history2_all.npy\"))\n","# directorybase = (\"/content/drive/MyDrive/PBCDcode/Cifar10Resnet18/data/activations/01network\")\n","# if not os.path.exists(directorybase):\n","#   os.makedirs(directorybase)\n","# directory_1 = os.path.join(directorybase, \"historytrain_all\"+'.npy')\n","# directory_2 = os.path.join(directorybase, \"historytest_all\"+'.npy')\n","# np.save(directory_1, history1_all)\n","# np.save(directory_2, history2_all)\n","# print(\"The final best acc of all is: {}, super parameter1 is: {}:\".format( best_acc_of_all, best_para1))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1690119705059,"user":{"displayName":"Zhang Hui","userId":"06808414998828253018"},"user_tz":-480},"id":"ATGu-5Q_-op9","outputId":"0d0a6fe8-bae7-413f-cc20-7c753f366319"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0.81230003 0.81230003 0.81230003 0.81230003 0.81230003]\n"," [0.8125     0.8125     0.8125     0.8125     0.8125    ]\n"," [0.81260002 0.81260002 0.81260002 0.81260002 0.81260002]\n"," [0.81279999 0.81279999 0.81279999 0.81279999 0.81279999]\n"," [0.8132     0.8132     0.8132     0.8132     0.8132    ]\n"," [0.81339997 0.81339997 0.81339997 0.81339997 0.81339997]\n"," [0.8136     0.8136     0.8136     0.8136     0.8136    ]\n"," [0.8136     0.8136     0.8136     0.8136     0.8136    ]\n"," [0.81349999 0.81349999 0.81349999 0.81349999 0.81349999]\n"," [0.81379998 0.81379998 0.81379998 0.81379998 0.81379998]\n"," [0.81400001 0.81400001 0.81400001 0.81400001 0.81400001]\n"," [0.8143     0.8143     0.8143     0.8143     0.8143    ]\n"," [0.81449997 0.81449997 0.81449997 0.81449997 0.81449997]\n"," [0.81470001 0.81470001 0.81470001 0.81470001 0.81470001]\n"," [0.81480002 0.81480002 0.81480002 0.81480002 0.81480002]\n"," [0.81529999 0.81529999 0.81529999 0.81529999 0.81529999]\n"," [0.81550002 0.81550002 0.81550002 0.81550002 0.81550002]\n"," [0.81569999 0.81569999 0.81569999 0.81569999 0.81569999]\n"," [0.81550002 0.81550002 0.81550002 0.81550002 0.81550002]\n"," [0.81529999 0.81529999 0.81529999 0.81529999 0.81529999]\n"," [0.81559998 0.81559998 0.81559998 0.81559998 0.81559998]\n"," [0.81580001 0.81580001 0.81580001 0.81580001 0.81580001]\n"," [0.81580001 0.81580001 0.81580001 0.81580001 0.81580001]\n"," [0.81580001 0.81580001 0.81580001 0.81580001 0.81580001]\n"," [0.81580001 0.81580001 0.81580001 0.81580001 0.81580001]\n"," [0.8161     0.8161     0.8161     0.8161     0.8161    ]\n"," [0.81580001 0.81580001 0.81580001 0.81580001 0.81580001]\n"," [0.81590003 0.81590003 0.81590003 0.81590003 0.81590003]\n"," [0.81569999 0.81569999 0.81569999 0.81569999 0.81569999]\n"," [0.81559998 0.81559998 0.81559998 0.81559998 0.81559998]\n"," [0.8154     0.8154     0.8154     0.8154     0.8154    ]\n"," [0.81489998 0.81489998 0.81489998 0.81489998 0.81489998]\n"," [0.81489998 0.81489998 0.81489998 0.81489998 0.81489998]\n"," [0.81489998 0.81489998 0.81489998 0.81489998 0.81489998]\n"," [0.81519997 0.81519997 0.81519997 0.81519997 0.81519997]\n"," [0.8154     0.8154     0.8154     0.8154     0.8154    ]\n"," [0.81550002 0.81550002 0.81550002 0.81550002 0.81550002]\n"," [0.8154     0.8154     0.8154     0.8154     0.8154    ]\n"," [0.8154     0.8154     0.8154     0.8154     0.8154    ]\n"," [0.8154     0.8154     0.8154     0.8154     0.8154    ]\n"," [0.         0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.        ]]\n"]}],"source":["directorybase = (\"/content/drive/MyDrive/PBCDcode/Cifar10Resnet18/data/activations/01network\")\n","historytrain_all = np.load(os.path.join(directorybase, \"historytrain_all.npy\"))\n","historytest_all =  np.load(os.path.join(directorybase, \"historytest_all.npy\"))\n","print(historytest_all)\n"]},{"cell_type":"markdown","metadata":{"id":"gDjDUOhmIIZE"},"source":["# Others"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["tT2VMZJXYhTw","oUxdmTzZ0uhW","6hHrNxGDAn_6","W_6tSL9AhFNr"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}